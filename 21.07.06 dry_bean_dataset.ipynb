{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74938d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\programdata\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.13.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.17.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.19.5)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.34.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (52.0.0.post20210125)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (1.32.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7a3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2ac330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd98e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6e53e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind = 'train'):\n",
    "    labels_path = os.path.join(path, \"%s-labels-idx1-ubyte\"%kind)\n",
    "    images_path = os.path.join(path, \"%s-images-idx3-ubyte\"%kind)\n",
    "    # label\n",
    "    with open(labels_path, \"rb\") as la_path:\n",
    "        magic, n = struct.unpack(\">II\", la_path.read(8))\n",
    "        labels = np.fromfile(la_path, dtype = np.uint8)\n",
    "    \n",
    "    # image\n",
    "    with open(images_path, \"rb\") as img_path:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", img_path.read(16))\n",
    "        images = np.fromfile(img_path, dtype = np.uint8).reshape(len(labels), 28**2)\n",
    "        images = ((images/255)-0.5)*2\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea61fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784\n",
      "10000 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist(\"C:/Users/CPB06GameN/Desktop/image_label_ubyte\", kind = \"train\")\n",
    "X_test, y_test = load_mnist(\"C:/Users/CPB06GameN/Desktop/image_label_ubyte\", kind = \"t10k\")\n",
    "print(X_train.shape[0], X_train.shape[1])\n",
    "print(X_test.shape[0], X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "535118ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "X_train_centered = (X_train-mean_vals)/std_val\n",
    "X_test_centered = (X_test-mean_vals)/std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a6af80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "y_train_onehot[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67a6c0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e02622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 50,\n",
    "        input_dim = X_train_centered.shape[1],\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros',\n",
    "        activation = \"tanh\"\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 50,\n",
    "        input_dim = 50,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros',\n",
    "        activation = \"tanh\"\n",
    "))\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 10,\n",
    "        input_dim = 50,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros',\n",
    "        activation = \"softmax\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "434deb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63ad6cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.0001, decay=1e-7, momentum=0.9)\n",
    "model.compile(optimizer=sgd_optimizer, loss =\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dac25a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 1.5616 - val_loss: 1.0417\n",
      "Epoch 2/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.9321 - val_loss: 0.7480\n",
      "Epoch 3/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.7384 - val_loss: 0.6122\n",
      "Epoch 4/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.6350 - val_loss: 0.5322\n",
      "Epoch 5/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.5691 - val_loss: 0.4790\n",
      "Epoch 6/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.5226 - val_loss: 0.4407\n",
      "Epoch 7/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.4877 - val_loss: 0.4115\n",
      "Epoch 8/50\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 0.4603 - val_loss: 0.3885\n",
      "Epoch 9/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.4380 - val_loss: 0.3697\n",
      "Epoch 10/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.4195 - val_loss: 0.3540\n",
      "Epoch 11/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.4036 - val_loss: 0.3406\n",
      "Epoch 12/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3899 - val_loss: 0.3289\n",
      "Epoch 13/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.3779 - val_loss: 0.3187\n",
      "Epoch 14/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.3672 - val_loss: 0.3096\n",
      "Epoch 15/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3576 - val_loss: 0.3016\n",
      "Epoch 16/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.3489 - val_loss: 0.2943\n",
      "Epoch 17/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.3409 - val_loss: 0.2876\n",
      "Epoch 18/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3336 - val_loss: 0.2815\n",
      "Epoch 19/50\n",
      "844/844 [==============================] - 1s 937us/step - loss: 0.3268 - val_loss: 0.2760\n",
      "Epoch 20/50\n",
      "844/844 [==============================] - 1s 993us/step - loss: 0.3206 - val_loss: 0.2708\n",
      "Epoch 21/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3147 - val_loss: 0.2660\n",
      "Epoch 22/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3092 - val_loss: 0.2616\n",
      "Epoch 23/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3040 - val_loss: 0.2573\n",
      "Epoch 24/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2992 - val_loss: 0.2535\n",
      "Epoch 25/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2946 - val_loss: 0.2498\n",
      "Epoch 26/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2902 - val_loss: 0.2463\n",
      "Epoch 27/50\n",
      "844/844 [==============================] - 2s 3ms/step - loss: 0.2860 - val_loss: 0.2430\n",
      "Epoch 28/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2820 - val_loss: 0.2399\n",
      "Epoch 29/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2782 - val_loss: 0.2370\n",
      "Epoch 30/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2746 - val_loss: 0.2341\n",
      "Epoch 31/50\n",
      "844/844 [==============================] - 2s 3ms/step - loss: 0.2711 - val_loss: 0.2314\n",
      "Epoch 32/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2678 - val_loss: 0.2288\n",
      "Epoch 33/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2645 - val_loss: 0.2263\n",
      "Epoch 34/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.2614 - val_loss: 0.2239\n",
      "Epoch 35/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2584 - val_loss: 0.2217\n",
      "Epoch 36/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2555 - val_loss: 0.2194\n",
      "Epoch 37/50\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 0.2527 - val_loss: 0.2173\n",
      "Epoch 38/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.2500 - val_loss: 0.2153\n",
      "Epoch 39/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.2474 - val_loss: 0.2133\n",
      "Epoch 40/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.2449 - val_loss: 0.2115\n",
      "Epoch 41/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.2424 - val_loss: 0.2096\n",
      "Epoch 42/50\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.2400 - val_loss: 0.2078\n",
      "Epoch 43/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2377 - val_loss: 0.2061\n",
      "Epoch 44/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2354 - val_loss: 0.2045\n",
      "Epoch 45/50\n",
      "844/844 [==============================] - 2s 3ms/step - loss: 0.2332 - val_loss: 0.2028\n",
      "Epoch 46/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2310 - val_loss: 0.2013\n",
      "Epoch 47/50\n",
      "844/844 [==============================] - 1s 2ms/step - loss: 0.2289 - val_loss: 0.1998\n",
      "Epoch 48/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2269 - val_loss: 0.1983\n",
      "Epoch 49/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2249 - val_loss: 0.1969\n",
      "Epoch 50/50\n",
      "844/844 [==============================] - 2s 2ms/step - loss: 0.2230 - val_loss: 0.1955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20bbc695ac0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_centered, y_train_onehot, batch_size=64, epochs=50, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f2d1c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "y_train_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92a4d2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58099726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "y_test_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99752325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56356\n",
      "0.9392666666666667\n"
     ]
    }
   ],
   "source": [
    "total_predicts = np.sum(y_train==y_train_pred, axis=0)\n",
    "print(total_predicts)\n",
    "train_res = total_predicts/y_train.shape[0]\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19a828ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56356\n",
      "0.9392666666666667\n"
     ]
    }
   ],
   "source": [
    "total_predicts = np.sum(y_train==y_train_pred, axis=0)\n",
    "print(total_predicts)\n",
    "train_res = total_predicts/y_train.shape[0]\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23564578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5 6\n",
      "33 4 6\n",
      "46 1 3\n",
      "63 3 2\n",
      "66 6 7\n",
      "92 9 4\n",
      "124 7 4\n",
      "149 2 9\n",
      "187 5 3\n",
      "217 6 5\n",
      "233 8 7\n",
      "241 9 8\n",
      "245 3 6\n",
      "247 4 2\n",
      "259 6 0\n",
      "300 4 6\n",
      "313 3 5\n",
      "320 9 7\n",
      "321 2 7\n",
      "340 5 3\n",
      "352 5 0\n",
      "362 2 7\n",
      "372 0 3\n",
      "380 0 5\n",
      "403 8 5\n",
      "444 2 8\n",
      "445 6 0\n",
      "447 4 9\n",
      "448 9 8\n",
      "449 3 5\n",
      "464 3 7\n",
      "478 5 8\n",
      "479 9 3\n",
      "495 8 0\n",
      "502 5 3\n",
      "507 3 5\n",
      "511 4 1\n",
      "530 9 4\n",
      "531 3 1\n",
      "542 8 4\n",
      "543 8 3\n",
      "551 7 1\n",
      "565 4 9\n",
      "578 3 2\n",
      "591 8 3\n",
      "619 1 8\n",
      "628 3 9\n",
      "629 2 6\n",
      "659 2 8\n",
      "684 7 3\n",
      "691 8 4\n",
      "707 4 9\n",
      "710 5 8\n",
      "717 0 6\n",
      "720 5 2\n",
      "728 2 8\n",
      "740 4 9\n",
      "741 2 8\n",
      "760 4 9\n",
      "791 5 9\n",
      "839 8 3\n",
      "844 8 7\n",
      "857 5 3\n",
      "881 4 9\n",
      "882 9 7\n",
      "898 7 2\n",
      "924 2 7\n",
      "939 2 0\n",
      "947 8 9\n",
      "950 7 2\n",
      "951 5 4\n",
      "965 6 0\n",
      "982 3 8\n",
      "998 8 9\n",
      "999 9 7\n",
      "1003 5 3\n",
      "1014 6 5\n",
      "1032 5 8\n",
      "1039 7 9\n",
      "1044 6 8\n",
      "1062 3 7\n",
      "1068 8 4\n",
      "1082 5 3\n",
      "1089 5 0\n",
      "1101 8 2\n",
      "1107 9 3\n",
      "1112 4 6\n",
      "1114 3 8\n",
      "1124 8 7\n",
      "1173 7 9\n",
      "1181 6 1\n",
      "1191 0 6\n",
      "1192 9 4\n",
      "1194 7 9\n",
      "1198 8 4\n",
      "1202 8 5\n",
      "1204 3 8\n",
      "1206 7 8\n",
      "1208 3 9\n",
      "1226 7 2\n",
      "1232 9 4\n",
      "1234 8 5\n",
      "1242 4 9\n",
      "1247 9 0\n",
      "1256 2 3\n",
      "1260 7 1\n",
      "1283 7 2\n",
      "1289 5 4\n",
      "1299 5 7\n",
      "1319 8 3\n",
      "1325 8 6\n",
      "1326 7 2\n",
      "1328 7 9\n",
      "1337 2 6\n",
      "1364 8 2\n",
      "1378 5 6\n",
      "1393 5 3\n",
      "1410 2 6\n",
      "1433 8 3\n",
      "1444 6 7\n",
      "1466 5 3\n",
      "1467 5 9\n",
      "1469 3 6\n",
      "1500 7 1\n",
      "1522 7 9\n",
      "1525 5 0\n",
      "1527 1 6\n",
      "1530 8 7\n",
      "1549 4 6\n",
      "1553 9 3\n",
      "1559 9 3\n",
      "1569 6 4\n",
      "1581 7 2\n",
      "1587 6 5\n",
      "1609 2 4\n",
      "1621 0 6\n",
      "1678 2 0\n",
      "1681 3 7\n",
      "1686 8 5\n",
      "1696 2 1\n",
      "1709 9 5\n",
      "1717 8 0\n",
      "1718 7 3\n",
      "1722 2 6\n",
      "1737 5 3\n",
      "1751 4 2\n",
      "1754 7 2\n",
      "1772 7 4\n",
      "1773 1 8\n",
      "1782 8 2\n",
      "1790 2 9\n",
      "1800 6 4\n",
      "1808 0 4\n",
      "1813 8 5\n",
      "1828 3 7\n",
      "1850 8 7\n",
      "1857 6 4\n",
      "1865 4 9\n",
      "1871 2 8\n",
      "1878 8 3\n",
      "1883 7 9\n",
      "1901 9 4\n",
      "1917 5 8\n",
      "1930 2 4\n",
      "1938 4 6\n",
      "1941 7 4\n",
      "1942 8 5\n",
      "1952 9 5\n",
      "1968 8 1\n",
      "1970 5 3\n",
      "1973 8 5\n",
      "1981 6 4\n",
      "1984 2 0\n",
      "2001 5 8\n",
      "2016 7 2\n",
      "2024 7 9\n",
      "2035 5 3\n",
      "2040 5 6\n",
      "2043 4 8\n",
      "2044 2 7\n",
      "2052 8 4\n",
      "2053 4 9\n",
      "2070 7 9\n",
      "2093 8 1\n",
      "2098 2 0\n",
      "2109 3 7\n",
      "2115 7 0\n",
      "2118 6 0\n",
      "2129 9 2\n",
      "2130 4 9\n",
      "2131 6 2\n",
      "2135 6 1\n",
      "2138 2 8\n",
      "2168 8 2\n",
      "2182 1 2\n",
      "2185 0 5\n",
      "2186 2 3\n",
      "2189 9 1\n",
      "2215 6 5\n",
      "2266 1 6\n",
      "2272 8 0\n",
      "2293 9 6\n",
      "2299 2 7\n",
      "2305 3 8\n",
      "2325 7 2\n",
      "2351 3 0\n",
      "2369 5 9\n",
      "2371 4 9\n",
      "2378 0 2\n",
      "2387 9 1\n",
      "2393 8 3\n",
      "2395 8 3\n",
      "2406 9 1\n",
      "2414 9 4\n",
      "2422 6 7\n",
      "2425 8 3\n",
      "2433 2 1\n",
      "2460 5 8\n",
      "2488 2 4\n",
      "2516 9 8\n",
      "2526 5 3\n",
      "2556 5 2\n",
      "2573 5 8\n",
      "2574 5 7\n",
      "2598 8 2\n",
      "2607 7 1\n",
      "2610 2 8\n",
      "2617 8 7\n",
      "2631 0 6\n",
      "2635 2 7\n",
      "2648 9 0\n",
      "2650 8 5\n",
      "2654 6 1\n",
      "2670 5 8\n",
      "2684 3 7\n",
      "2695 7 4\n",
      "2713 0 8\n",
      "2730 7 4\n",
      "2770 3 6\n",
      "2771 4 9\n",
      "2780 2 3\n",
      "2810 5 0\n",
      "2812 9 4\n",
      "2832 5 3\n",
      "2850 5 3\n",
      "2877 4 7\n",
      "2896 8 0\n",
      "2906 3 5\n",
      "2925 5 0\n",
      "2927 3 2\n",
      "2945 3 7\n",
      "2946 1 8\n",
      "2953 3 5\n",
      "2995 6 5\n",
      "3005 9 1\n",
      "3060 9 7\n",
      "3073 1 2\n",
      "3114 4 6\n",
      "3117 5 9\n",
      "3130 6 0\n",
      "3133 4 9\n",
      "3136 7 1\n",
      "3145 5 9\n",
      "3157 5 7\n",
      "3167 3 1\n",
      "3189 7 4\n",
      "3206 8 3\n",
      "3240 9 3\n",
      "3262 7 8\n",
      "3269 6 2\n",
      "3284 8 7\n",
      "3289 8 5\n",
      "3316 7 4\n",
      "3329 7 2\n",
      "3330 2 3\n",
      "3381 3 2\n",
      "3384 2 5\n",
      "3422 6 0\n",
      "3436 2 1\n",
      "3456 3 8\n",
      "3490 4 9\n",
      "3503 9 5\n",
      "3520 6 4\n",
      "3549 3 2\n",
      "3558 5 0\n",
      "3559 8 5\n",
      "3565 5 8\n",
      "3567 8 5\n",
      "3573 7 2\n",
      "3585 7 3\n",
      "3597 9 3\n",
      "3598 1 8\n",
      "3604 7 0\n",
      "3612 4 9\n",
      "3629 8 3\n",
      "3662 8 5\n",
      "3681 2 3\n",
      "3702 5 3\n",
      "3716 9 3\n",
      "3718 4 9\n",
      "3727 8 4\n",
      "3730 7 9\n",
      "3749 6 0\n",
      "3751 7 1\n",
      "3757 8 3\n",
      "3763 5 7\n",
      "3767 7 2\n",
      "3776 5 8\n",
      "3780 4 6\n",
      "3796 2 8\n",
      "3801 6 0\n",
      "3808 7 3\n",
      "3811 2 3\n",
      "3817 2 4\n",
      "3821 9 4\n",
      "3836 7 9\n",
      "3838 7 1\n",
      "3846 6 4\n",
      "3853 6 5\n",
      "3855 5 0\n",
      "3862 2 3\n",
      "3869 9 4\n",
      "3876 2 8\n",
      "3893 5 6\n",
      "3902 5 3\n",
      "3906 1 3\n",
      "3926 9 3\n",
      "3941 4 6\n",
      "3946 2 8\n",
      "3951 8 5\n",
      "3962 3 4\n",
      "3968 5 3\n",
      "3976 7 1\n",
      "3985 9 4\n",
      "3988 8 5\n",
      "4000 9 4\n",
      "4017 4 9\n",
      "4059 5 8\n",
      "4063 6 5\n",
      "4065 0 6\n",
      "4072 5 3\n",
      "4075 8 0\n",
      "4078 9 7\n",
      "4093 9 4\n",
      "4131 5 1\n",
      "4140 8 2\n",
      "4152 5 1\n",
      "4154 9 4\n",
      "4156 2 8\n",
      "4163 9 0\n",
      "4176 2 4\n",
      "4180 2 3\n",
      "4199 7 9\n",
      "4201 1 7\n",
      "4205 2 6\n",
      "4211 6 5\n",
      "4212 1 3\n",
      "4224 9 7\n",
      "4238 7 9\n",
      "4248 2 8\n",
      "4271 5 3\n",
      "4284 9 3\n",
      "4289 2 7\n",
      "4300 5 8\n",
      "4302 5 8\n",
      "4306 3 7\n",
      "4313 4 9\n",
      "4315 5 8\n",
      "4344 9 7\n",
      "4355 5 9\n",
      "4374 5 6\n",
      "4380 8 5\n",
      "4419 8 5\n",
      "4425 9 4\n",
      "4427 2 8\n",
      "4433 7 3\n",
      "4435 3 7\n",
      "4449 6 0\n",
      "4451 2 8\n",
      "4497 8 4\n",
      "4498 7 9\n",
      "4500 9 1\n",
      "4523 8 3\n",
      "4536 6 5\n",
      "4540 7 8\n",
      "4567 4 9\n",
      "4571 6 2\n",
      "4575 4 2\n",
      "4578 7 9\n",
      "4601 8 4\n",
      "4615 2 4\n",
      "4630 3 5\n",
      "4639 8 3\n",
      "4640 8 3\n",
      "4690 7 2\n",
      "4724 8 0\n",
      "4731 8 7\n",
      "4739 0 4\n",
      "4751 4 6\n",
      "4761 9 1\n",
      "4785 3 8\n",
      "4807 8 3\n",
      "4808 3 5\n",
      "4814 6 0\n",
      "4823 9 4\n",
      "4837 7 2\n",
      "4874 9 0\n",
      "4876 2 6\n",
      "4880 0 8\n",
      "4886 7 1\n",
      "4890 8 5\n",
      "4915 5 8\n",
      "4939 2 3\n",
      "4950 2 3\n",
      "4952 6 5\n",
      "4956 8 4\n",
      "4966 7 1\n",
      "4990 3 2\n",
      "5009 9 4\n",
      "5067 3 2\n",
      "5068 4 1\n",
      "5078 3 2\n",
      "5135 9 4\n",
      "5140 3 2\n",
      "5143 3 5\n",
      "5165 0 5\n",
      "5176 8 5\n",
      "5183 8 4\n",
      "5210 9 7\n",
      "5246 7 2\n",
      "5265 6 4\n",
      "5331 1 6\n",
      "5457 1 8\n",
      "5495 8 3\n",
      "5600 7 9\n",
      "5608 5 6\n",
      "5611 8 6\n",
      "5617 4 9\n",
      "5620 7 9\n",
      "5634 2 0\n",
      "5642 1 5\n",
      "5677 4 6\n",
      "5678 8 5\n",
      "5709 7 9\n",
      "5714 7 9\n",
      "5734 3 7\n",
      "5749 8 6\n",
      "5757 9 7\n",
      "5842 4 7\n",
      "5858 7 9\n",
      "5887 7 0\n",
      "5888 4 0\n",
      "5891 5 3\n",
      "5913 5 3\n",
      "5922 5 3\n",
      "5936 4 9\n",
      "5937 5 3\n",
      "5945 3 8\n",
      "5955 3 8\n",
      "5957 5 8\n",
      "5972 5 3\n",
      "5973 3 8\n",
      "5985 5 8\n",
      "6023 3 5\n",
      "6035 2 0\n",
      "6037 4 9\n",
      "6042 5 3\n",
      "6043 5 3\n",
      "6046 3 8\n",
      "6059 3 8\n",
      "6065 3 8\n",
      "6071 9 3\n",
      "6081 9 8\n",
      "6091 9 0\n",
      "6093 2 8\n",
      "6109 2 1\n",
      "6157 9 0\n",
      "6166 9 3\n",
      "6168 9 3\n",
      "6172 9 0\n",
      "6173 9 0\n",
      "6324 5 2\n",
      "6347 8 6\n",
      "6385 5 2\n",
      "6391 2 6\n",
      "6400 0 6\n",
      "6421 3 2\n",
      "6425 6 2\n",
      "6426 0 6\n",
      "6480 2 6\n",
      "6505 9 0\n",
      "6555 8 7\n",
      "6560 9 3\n",
      "6564 3 7\n",
      "6568 9 4\n",
      "6569 3 9\n",
      "6570 3 9\n",
      "6571 9 7\n",
      "6597 0 7\n",
      "6598 5 2\n",
      "6603 8 7\n",
      "6608 9 5\n",
      "6625 8 7\n",
      "6632 9 5\n",
      "6641 8 5\n",
      "6642 9 5\n",
      "6651 0 4\n",
      "6706 5 7\n",
      "6721 2 4\n",
      "6740 9 0\n",
      "6744 2 6\n",
      "6746 5 4\n",
      "6765 8 5\n",
      "6775 5 8\n",
      "6785 2 6\n",
      "6847 6 4\n",
      "6872 4 6\n",
      "6926 6 4\n",
      "6945 8 9\n",
      "7043 9 7\n",
      "7094 8 9\n",
      "7107 9 7\n",
      "7121 8 9\n",
      "7130 3 7\n",
      "7208 8 7\n",
      "7216 0 7\n",
      "7220 8 3\n",
      "7248 3 7\n",
      "7259 8 7\n",
      "7338 4 9\n",
      "7432 7 2\n",
      "7434 4 8\n",
      "7451 5 6\n",
      "7459 9 5\n",
      "7492 2 7\n",
      "7498 5 3\n",
      "7545 8 9\n",
      "7637 2 3\n",
      "7692 3 5\n",
      "7797 5 6\n",
      "7800 3 2\n",
      "7812 1 8\n",
      "7821 3 2\n",
      "7842 5 8\n",
      "7849 3 2\n",
      "7850 5 8\n",
      "7851 6 0\n",
      "7857 2 9\n",
      "7858 3 2\n",
      "7859 5 8\n",
      "7870 5 8\n",
      "7886 2 4\n",
      "7888 5 4\n",
      "7905 3 2\n",
      "7917 2 4\n",
      "7918 5 8\n",
      "7921 8 1\n",
      "7945 2 6\n",
      "7990 1 8\n",
      "8020 1 8\n",
      "8062 5 8\n",
      "8072 5 3\n",
      "8091 2 8\n",
      "8094 2 8\n",
      "8095 4 8\n",
      "8183 8 5\n",
      "8246 3 9\n",
      "8258 8 6\n",
      "8272 3 8\n",
      "8277 3 5\n",
      "8279 8 6\n",
      "8294 8 5\n",
      "8308 3 5\n",
      "8326 6 5\n",
      "8339 8 6\n",
      "8406 4 9\n",
      "8408 8 6\n",
      "8426 9 4\n",
      "8520 4 9\n",
      "8522 8 6\n",
      "8553 5 3\n",
      "9009 7 2\n",
      "9010 2 8\n",
      "9015 7 2\n",
      "9019 7 2\n",
      "9022 3 2\n",
      "9024 7 2\n",
      "9026 9 4\n",
      "9036 7 2\n",
      "9045 7 2\n",
      "9071 1 8\n",
      "9182 3 9\n",
      "9214 9 4\n",
      "9280 8 5\n",
      "9316 8 9\n",
      "9422 5 3\n",
      "9427 5 3\n",
      "9446 2 6\n",
      "9465 5 3\n",
      "9482 5 3\n",
      "9534 7 9\n",
      "9538 4 9\n",
      "9544 9 7\n",
      "9587 9 4\n",
      "9595 2 8\n",
      "9624 3 8\n",
      "9634 0 3\n",
      "9642 9 7\n",
      "9643 1 7\n",
      "9664 2 7\n",
      "9677 0 5\n",
      "9679 6 5\n",
      "9698 6 5\n",
      "9700 2 6\n",
      "9716 2 0\n",
      "9719 5 0\n",
      "9729 5 6\n",
      "9732 8 5\n",
      "9735 4 9\n",
      "9740 9 4\n",
      "9744 8 1\n",
      "9745 4 2\n",
      "9749 5 6\n",
      "9752 2 0\n",
      "9768 2 0\n",
      "9770 5 0\n",
      "9777 5 0\n",
      "9779 2 0\n",
      "9792 4 9\n",
      "9808 9 4\n",
      "9811 2 8\n",
      "9839 2 7\n",
      "9856 9 5\n",
      "9858 6 8\n",
      "9867 2 8\n",
      "9874 2 8\n",
      "9879 0 2\n",
      "9883 5 6\n",
      "9890 9 4\n",
      "9891 9 7\n",
      "9893 2 8\n",
      "9904 2 0\n",
      "9905 3 7\n",
      "9925 3 2\n",
      "9941 5 6\n",
      "9944 3 9\n",
      "9959 8 1\n",
      "9970 5 3\n",
      "9975 3 2\n",
      "9982 5 2\n",
      "9986 3 8\n",
      "651\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y != y_test_pred[i]:\n",
    "        print(i, y, y_test_pred[i])\n",
    "        n+=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d3cd3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "image = np.reshape(X_test[12], [28, 28])\n",
    "plt.imshow(image, cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fd1be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,111,946\n",
      "Trainable params: 1,111,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5,5), padding = 'valid', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPool2D(2,2))\n",
    "model.add(layers.Conv2D(64, (5,5), padding = 'valid', activation='relu'))\n",
    "model.add(layers.MaxPool2D(2,2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6ee05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
